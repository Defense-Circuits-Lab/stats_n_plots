{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149c414-d66f-4cb4-b454-75c618c4f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp stats_refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc2ab9-a1f1-4a43-9711-8531ae871a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce627c-5b97-40a3-98c0-461bd1528cc1",
   "metadata": {},
   "source": [
    "# stats_refactored\n",
    "\n",
    "> This module contains all functions to compute the relevant statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca460ca-63fe-431b-97f7-08f5bf90d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import itertools\n",
    "\n",
    "from dcl_stats_n_plots.database import Database\n",
    "from dcl_stats_n_plots.plots_refactored import OneSamplePlots, MultipleIndependentSamplesPlots, MixedModelANOVAPlots, MultipleDependentSamplesPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e1f58-c598-496c-86b1-715f842faf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StatisticalTest(ABC):\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name_displayed_in_gui(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def plot_handler(self):\n",
    "        # set the corresponding plot_handler class\n",
    "        pass\n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    def add_test_specific_information_to_df_infos(self) -> Dict:\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        # add additional infos here like:\n",
    "        # df_infos['additional_info'] = value\n",
    "        # or leave it like this / remove these comments, if there is nothing to add\n",
    "        return df_infos\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_test_specific_information_to_summary_stats(self) -> Dict:\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        # add additional infos here like:\n",
    "        # summary_stats['additional_info'] = value\n",
    "        # keys that have to be added:\n",
    "        # 'performed_test': str\n",
    "        # 'full_test_results': pd.DataFrame\n",
    "        # 'p-value': float\n",
    "        # 'stars_str': str\n",
    "        return summary_stats\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_test_specific_information_to_pairwise_comparisons(self) -> Dict:\n",
    "        pairwise_comparisons = self.lut['pairwise_comparisons'].copy()\n",
    "        # add additional infos here like:\n",
    "        # pairwise_comparisons['additional_info'] = value\n",
    "        # or leave it like this / remove these comments, if there is nothing to add\n",
    "        return pairwise_comparisons\n",
    "    \n",
    "    \n",
    "    def compute(self, database: Database) -> Database:\n",
    "        self.df = database.data.copy()\n",
    "        self.lut = self.initialize_lut_with_basic_df_infos()\n",
    "        self.lut['df_infos'] = self.add_test_specific_information_to_df_infos()\n",
    "        for group_id in self.lut['df_infos']['all_group_ids']:\n",
    "            if 'session_column_name' in self.lut['df_infos'].keys():\n",
    "                for session_id in self.lut['df_infos']['all_session_ids']:\n",
    "                    self.lut['group_level_stats'][(group_id, session_id)] = self.add_normality_check_on_group_level(group_id = group_id, session_id = session_id)\n",
    "            else:\n",
    "                self.lut['group_level_stats'][group_id] = self.add_normality_check_on_group_level(group_id = group_id)\n",
    "        self.lut['summary_stats'] = self.initialize_summary_stats()\n",
    "        self.lut['summary_stats'] = self.add_test_specific_information_to_summary_stats()\n",
    "        self.lut['pairwise_comparisons'] = self.add_test_specific_information_to_pairwise_comparisons()\n",
    "        database.stats_results = self.lut.copy()\n",
    "        return database\n",
    "\n",
    "\n",
    "    def initialize_lut_with_basic_df_infos(self) -> Dict:\n",
    "        lut = {'df_infos': dict(),\n",
    "               'summary_stats': dict(),\n",
    "               'group_level_stats': dict(),\n",
    "               'pairwise_comparisons': dict()}\n",
    "        df_infos = {'data_column_name': self.df.columns[0],\n",
    "                    'data_column_values': self.df.iloc[:, 0].values.copy(),\n",
    "                    'group_column_name': self.df.columns[1],\n",
    "                    'group_column_values': self.df.iloc[:, 1].values.copy(),\n",
    "                    'all_group_ids': list(self.df.iloc[:, 1].unique()),\n",
    "                    'n_groups': len(list(self.df.iloc[:, 1].unique()))}\n",
    "        lut['df_infos'] = df_infos\n",
    "        return lut\n",
    "\n",
    "\n",
    "    def add_normality_check_on_group_level(self, group_id: str, session_id: Optional[str]=None) -> Dict:\n",
    "        data_column_name = self.lut['df_infos']['data_column_name']\n",
    "        group_column_name = self.lut['df_infos']['group_column_name']\n",
    "        if type(session_id) == str:\n",
    "            session_column_name = self.lut['df_infos']['session_column_name']\n",
    "            data = self.df.loc[(self.df[group_column_name] == group_id)\n",
    "                               & (self.df[session_column_name] == session_id), data_column_name].values.copy()\n",
    "        else:\n",
    "            data = self.df.loc[self.df[group_column_name] == group_id, data_column_name].values.copy()\n",
    "        normality_test_results = pg.normality(data)\n",
    "        is_normally_distributed = normality_test_results['normal'][0]\n",
    "        normality_check = {'data': data,\n",
    "                           'normality_test_results': normality_test_results,\n",
    "                           'is_normally_distributed': is_normally_distributed}\n",
    "        return normality_check\n",
    "\n",
    "\n",
    "    def initialize_summary_stats(self) -> Dict:\n",
    "        are_all_normally_distributed = []\n",
    "        for group_id in self.lut['df_infos']['all_group_ids']:\n",
    "            if 'session_column_name' in self.lut['df_infos'].keys():\n",
    "                for session_id in self.lut['df_infos']['all_session_ids']:\n",
    "                    are_all_normally_distributed.append(self.lut['group_level_stats'][(group_id, session_id)]['is_normally_distributed'])\n",
    "            else:\n",
    "                are_all_normally_distributed.append(self.lut['group_level_stats'][group_id]['is_normally_distributed'])\n",
    "        if self.lut['df_infos']['n_groups'] > 1:\n",
    "            data_all_groups = []\n",
    "            for group_id in self.lut['df_infos']['all_group_ids']:\n",
    "                if 'session_column_name' in self.lut['df_infos'].keys():\n",
    "                    for session_id in self.lut['df_infos']['all_session_ids']:\n",
    "                        data_all_groups.append(self.lut['group_level_stats'][(group_id, session_id)]['data'])\n",
    "                else:\n",
    "                    data_all_groups.append(self.lut['group_level_stats'][group_id]['data'])\n",
    "            homoscedasticity_test_results = pg.homoscedasticity(data_all_groups)\n",
    "            variance_is_equal = homoscedasticity_test_results['equal_var'].values[0]\n",
    "            if all(are_all_normally_distributed) and variance_is_equal:\n",
    "                use_parametric = True\n",
    "            else:\n",
    "                use_parametric = False\n",
    "            summary_stats = {'all_normally_distributed': all(are_all_normally_distributed),\n",
    "                             'homoscedasticity_test_results': homoscedasticity_test_results,\n",
    "                             'variance_is_equal': variance_is_equal,\n",
    "                             'use_parametric': use_parametric}\n",
    "        else:\n",
    "            if all(are_all_normally_distributed):\n",
    "                use_parametric = True\n",
    "            else:\n",
    "                use_parametric = False\n",
    "            summary_stats = {'all_normally_distributed': all(are_all_normally_distributed),\n",
    "                             'use_parametric': use_parametric}\n",
    "        return summary_stats\n",
    "\n",
    "\n",
    "    def get_stars_string(self, p: float) -> str:\n",
    "        if p <= 0.001:\n",
    "            stars_string = '***'\n",
    "        elif p <= 0.01:\n",
    "            stars_string = '**'\n",
    "        elif p <= 0.05:\n",
    "            stars_string = '*'\n",
    "        else:\n",
    "            stars_string = 'n.s.'\n",
    "        return stars_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2924cb-db6f-4368-b58a-8249c72d396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OneSampleStats(StatisticalTest):\n",
    "    \n",
    "    @property\n",
    "    def name_displayed_in_gui(self):\n",
    "        return 'One sample test: comparison of values from one sample to a reference value'\n",
    "\n",
    "    @property\n",
    "    def plot_handler(self):\n",
    "        return OneSamplePlots\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_df_infos(self) -> Dict:\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        df_infos['fixed_value_column_name'] = self.df.columns[2]\n",
    "        df_infos['fixed_value'] = self.df.iloc[0, 2]\n",
    "        df_infos['group_id'] = self.df.iloc[:, 1].unique()[0]\n",
    "        return df_infos\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_summary_stats(self) -> Dict:\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        data = self.lut['df_infos']['data_column_values']\n",
    "        fixed_value = self.lut['df_infos']['fixed_value']\n",
    "        if summary_stats['use_parametric']:\n",
    "            performed_test = 'one sample t-test'\n",
    "            full_test_results = pg.ttest(x = data, y = fixed_value, paired = False, alternative = 'two-sided')\n",
    "        else:\n",
    "            performed_test =  'one sample wilcoxon rank-sum test'\n",
    "            full_test_results = pg.wilcoxon(data - fixed_value, alternative = 'two-sided')\n",
    "        summary_stats['performed_test'] = performed_test\n",
    "        summary_stats['full_test_results'] = full_test_results\n",
    "        summary_stats['p_value'] = full_test_results['p-val'].values[0]\n",
    "        summary_stats['stars_str'] = self.get_stars_string(p = summary_stats['p_value'])      \n",
    "        return summary_stats\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_pairwise_comparisons(self) -> Dict:\n",
    "        pairwise_comparisons = self.lut['pairwise_comparisons'].copy()\n",
    "        # add additional infos here like:\n",
    "        # pairwise_comparisons['additional_info'] = value\n",
    "        # or leave it like this / remove these comments, if there is nothing to add\n",
    "        return pairwise_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629dcb0-c7cb-4045-a344-d0e8c762e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultipleIndependentSamplesStats(StatisticalTest):\n",
    "    \n",
    "    @property\n",
    "    def name_displayed_in_gui(self):\n",
    "        return 'Pairwise comparison of values from two or more *independent* samples'\n",
    "\n",
    "    @property\n",
    "    def plot_handler(self):\n",
    "        return MultipleIndependentSamplesPlots\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_df_infos(self) -> Dict:\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        return df_infos\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_summary_stats(self) -> Dict:\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        if len(df_infos['all_group_ids']) > 2:\n",
    "            if summary_stats['use_parametric']:\n",
    "                performed_test = 'One-way ANOVA'\n",
    "                full_test_results = pg.anova(data = self.df, dv = df_infos['data_column_name'], between = df_infos['group_column_name'])\n",
    "                p_value = full_test_results['p-unc'].values[0]\n",
    "            else:\n",
    "                performed_test = 'Kruskal-Wallis-ANOVA'\n",
    "                full_test_results = pg.kruskal(data = self.df, dv = df_infos['data_column_name'], between = df_infos['group_column_name'])\n",
    "                p_value = full_test_results['p-unc'].values[0]\n",
    "        if len(df_infos['all_group_ids']) == 2:\n",
    "            id_group_a, id_group_b = df_infos['all_group_ids']\n",
    "            group_column_name = df_infos['group_column_name']\n",
    "            data_group_a = self.df.loc[self.df[group_column_name] == id_group_a, df_infos['data_column_name']].values\n",
    "            data_group_b = self.df.loc[self.df[group_column_name] == id_group_b, df_infos['data_column_name']].values\n",
    "            if summary_stats['use_parametric']: \n",
    "                performed_test = 'unpaired two sample t-test (with Welch-correction if applicable)'\n",
    "                full_test_results = pg.ttest(x = data_group_a, y = data_group_b, alternative = 'two-sided', correction = 'auto')\n",
    "                p_value = full_test_results['p-val'].values[0]\n",
    "            else:\n",
    "                performed_test = 'Mann-Whitney U test'\n",
    "                full_test_results = pg.mwu(x = data_group_a, y = data_group_b, alternative = 'two-sided')\n",
    "                p_value = full_test_results['p-val'].values[0]\n",
    "        summary_stats['performed_test'] = performed_test\n",
    "        summary_stats['full_test_results'] = full_test_results\n",
    "        summary_stats['p_value'] = p_value\n",
    "        summary_stats['stars_str'] = self.get_stars_string(p = summary_stats['p_value'])      \n",
    "        return summary_stats            \n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_pairwise_comparisons(self) -> Dict:\n",
    "        pairwise_comparisons = self.lut['pairwise_comparisons'].copy()\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        df_infos = self.lut['df_infos'].copy() \n",
    "        pairwise_comparisons = pg.pairwise_ttests(data = self.df, dv = df_infos['data_column_name'], between = df_infos['group_column_name'], \n",
    "                                                  parametric = summary_stats['use_parametric'], padjust='holm')\n",
    "        return pairwise_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ccb6b-bbdc-4708-be7b-6765e18ba8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MixedModelANOVAStats(StatisticalTest):\n",
    "    \n",
    "    @property\n",
    "    def name_displayed_in_gui(self):\n",
    "        return 'Mixed-model ANOVA: compare two or more independent samples, all measured at several timepoints'\n",
    "\n",
    "    @property\n",
    "    def plot_handler(self):\n",
    "        return MixedModelANOVAPlots\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_df_infos(self) -> Dict:\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        df_infos['subject_column_name'] = self.df.columns[2]\n",
    "        df_infos['session_column_name'] = self.df.columns[3]\n",
    "        df_infos['all_session_ids'] = list(self.df[df_infos['session_column_name']].unique())\n",
    "        return df_infos\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_summary_stats(self) -> Dict:\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        for group_id in df_infos['all_group_ids']:\n",
    "            for session_id in df_infos['all_session_ids']:\n",
    "                self.lut['group_level_stats'][(group_id, session_id)]['mean'] = self.lut['group_level_stats'][(group_id, session_id)]['data'].mean()\n",
    "        if summary_stats['use_parametric'] == False:\n",
    "            user_warning_0 = 'Warning: Please be aware that the data require non-parametric testing.\\n'\n",
    "            user_warning_1 = 'However, this is not implemented yet and a parametric Mixed-model ANOVA is computed instead.'\n",
    "            print(user_warning_0 + user_warning_1)\n",
    "        summary_stats['performed_test'] = 'Mixed-model ANOVA'\n",
    "        full_test_results = pg.mixed_anova(data = self.df, dv = df_infos['data_column_name'], within = df_infos['session_column_name'], \n",
    "                                           subject = df_infos['subject_column_name'], between = df_infos['group_column_name'])\n",
    "        summary_stats['full_test_results'] = full_test_results\n",
    "        summary_stats['p_value'] = full_test_results['p-unc'].values[0] # group comparison (also possible: 1 -> session, 2 -> interaction)\n",
    "        summary_stats['stars_str'] = self.get_stars_string(p = summary_stats['p_value'])      \n",
    "        return summary_stats \n",
    "    \n",
    "\n",
    "    def add_test_specific_information_to_pairwise_comparisons(self) -> Dict:\n",
    "        pairwise_comparisons = self.lut['pairwise_comparisons'].copy()\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        pairwise_comparisons = pg.pairwise_ttests(data = self.df, dv = df_infos['data_column_name'], between = df_infos['group_column_name'], \n",
    "                                                  within = df_infos['session_column_name'], subject = df_infos['subject_column_name'],\n",
    "                                                  parametric = summary_stats['use_parametric'], padjust='holm')\n",
    "        return pairwise_comparisons  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4afe2-2b0e-4814-b709-5063923549b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultipleDependentSamplesStats(StatisticalTest):\n",
    "    \n",
    "    @property\n",
    "    def name_displayed_in_gui(self):\n",
    "        return 'Pairwise comparison of values from two or more *dependent* samples (i.e. data with repeated measurements)'\n",
    "\n",
    "    @property\n",
    "    def plot_handler(self):\n",
    "        return MultipleDependentSamplesPlots\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_df_infos(self) -> Dict:\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        df_infos['subject_column_name'] = self.df.columns[2]\n",
    "        self.raise_error_if_not_all_subjects_have_all_repeated_measures(tmp_df_info = df_infos)\n",
    "        return df_infos\n",
    "    \n",
    "    \n",
    "    def raise_error_if_not_all_subjects_have_all_repeated_measures(self, tmp_df_info: Dict) -> None:\n",
    "        group_column_name, subject_column_name = tmp_df_info['group_column_name'], tmp_df_info['subject_column_name']\n",
    "        first_group_id = tmp_df_info['all_group_ids'][0]\n",
    "        expected_subjects = list(self.df.loc[self.df[group_column_name] == first_group_id, subject_column_name].unique())\n",
    "        expected_subject_count = len(expected_subjects)\n",
    "        for additional_group_id in tmp_df_info['all_group_ids'][1:]:\n",
    "            subjects = list(self.df.loc[self.df[group_column_name] == additional_group_id, subject_column_name].unique())\n",
    "            subject_count = len(subjects)\n",
    "            count_matches = subject_count == expected_subject_count\n",
    "            subject_ids_match = all([True for elem in subjects if elem in expected_subjects])\n",
    "            if count_matches and subject_ids_match:\n",
    "                continue\n",
    "            else:\n",
    "                error_message1 = 'Repeated measurements are required from each subject for ALL sessions.\\n'\n",
    "                error_message2 = 'In your input data, however, it seems like there is something wrong with the data of\\n'\n",
    "                error_message3 = f'session \"{additional_group_id}\". So please check & revise your input data again!'\n",
    "                error_message = error_message1 + error_message2 + error_message3\n",
    "                raise ValueError(error_message)\n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_summary_stats(self) -> Dict:\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        df_infos = self.lut['df_infos'].copy()\n",
    "        if len(df_infos['all_group_ids']) > 2:\n",
    "            if summary_stats['use_parametric']:\n",
    "                performed_test = 'One-way repeated-measurements ANOVA'\n",
    "                full_test_results = pg.rm_anova(data = self.df, dv = df_infos['data_column_name'], within = df_infos['group_column_name'], subject = df_infos['subject_column_name'])\n",
    "                p_value = full_test_results['p-unc'].values[0]\n",
    "            else:\n",
    "                performed_test = 'Friedman-ANOVA'\n",
    "                full_test_results = pg.friedman(data = self.df, dv = df_infos['data_column_name'], within = df_infos['group_column_name'], subject = df_infos['subject_column_name'])\n",
    "                p_value = full_test_results['p-unc'].values[0]\n",
    "        if len(df_infos['all_group_ids']) == 2:\n",
    "            id_group_a, id_group_b = df_infos['all_group_ids']\n",
    "            group_column_name = df_infos['group_column_name']\n",
    "            self.df.sort_values(by = df_infos['subject_column_name'], inplace = True)\n",
    "            data_group_a = self.df.loc[self.df[group_column_name] == id_group_a, df_infos['data_column_name']].values\n",
    "            data_group_b = self.df.loc[self.df[group_column_name] == id_group_b, df_infos['data_column_name']].values\n",
    "            if summary_stats['use_parametric']: \n",
    "                performed_test = 'paired two sample t-test (with Welch-correction if applicable)'\n",
    "                full_test_results = pg.ttest(x = data_group_a, y = data_group_b, paired = True, alternative = 'two-sided', correction = 'auto')\n",
    "                p_value = full_test_results['p-val'].values[0]\n",
    "            else:\n",
    "                performed_test = 'Wilcoxon signed-rank test'\n",
    "                full_test_results = pg.wilcoxon(x = data_group_a, y = data_group_b, alternative = 'two-sided')\n",
    "                p_value = full_test_results['p-val'].values[0]            \n",
    "        summary_stats['performed_test'] = performed_test\n",
    "        summary_stats['full_test_results'] = full_test_results\n",
    "        summary_stats['p_value'] = p_value\n",
    "        summary_stats['stars_str'] = self.get_stars_string(p = summary_stats['p_value'])      \n",
    "        return summary_stats \n",
    "\n",
    "\n",
    "    def add_test_specific_information_to_pairwise_comparisons(self) -> Dict:\n",
    "        pairwise_comparisons = self.lut['pairwise_comparisons'].copy()\n",
    "        summary_stats = self.lut['summary_stats'].copy()\n",
    "        df_infos = self.lut['df_infos'].copy() \n",
    "        pairwise_comparisons = pg.pairwise_ttests(data = self.df, dv = df_infos['data_column_name'], within = df_infos['group_column_name'], \n",
    "                                                  subject = df_infos['subject_column_name'], parametric = summary_stats['use_parametric'], padjust='holm')\n",
    "        return pairwise_comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
